{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Model Architecture:\n",
        "The implemented model leverages a Transformer architecture for sentiment analysis. The Transformer block consists of multiple self-attention mechanisms applied iteratively (num_heads). Each attention mechanism captures different aspects of the input sequence, allowing the model to efficiently learn complex patterns. A subsequent 1D convolutional layer is employed to capture local patterns, followed by global average pooling for dimensionality reduction. Dropout is used for regularization, and the final layers include a dense layer with ReLU activation and a sigmoid activation for binary sentiment classification."
      ],
      "metadata": {
        "id": "cKHsZEtkSVvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Importing Libraries"
      ],
      "metadata": {
        "id": "3_CXv1I5SYYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Attention, LayerNormalization, Dropout"
      ],
      "metadata": {
        "id": "E6fpqoYPOW8A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load the Dataset\n",
        "Reads the IMDB dataset from a CSV file using Pandas and stores it in a DataFrame (df).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rxDIAZzqSmfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/IMDB Dataset.csv\")"
      ],
      "metadata": {
        "id": "cmdx05WCQ-sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3: Data Cleaning and Preprocessing\n",
        "\n",
        "* Cleans and preprocesses the text data in the 'review' column.\n",
        "* Removes non-alphabetic characters.\n",
        "* Converts text to lowercase.\n",
        "* Removes stopwords using NLTK."
      ],
      "metadata": {
        "id": "0h74aK-dT3MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['cleaned_review'] = df['review'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
        "df['cleaned_review'] = df['cleaned_review'].apply(lambda x: x.lower())\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['cleaned_review'] = df['cleaned_review'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))"
      ],
      "metadata": {
        "id": "edUDX3NjRA6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Data Splitting\n",
        "Splits the dataset into training and testing sets using the train_test_split function from scikit-learn."
      ],
      "metadata": {
        "id": "p4NYb5BPUFX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_review'], df['sentiment'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "RbH9GomiRDJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Label Encoding for Sentiment Column\n",
        "Encodes the categorical 'sentiment' labels into numerical format using LabelEncoder.\n",
        "\n"
      ],
      "metadata": {
        "id": "yL4OgoErUFrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Encoding for Sentiment Column\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "5ryFfHWIRFL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Tokenization and Padding\n",
        "Tokenizes and pads the text sequences for input into a neural network using Keras's Tokenizer and pad_sequences.\n"
      ],
      "metadata": {
        "id": "_RaWtZm-U0yI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=100, truncating='post', padding='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=100, truncating='post', padding='post')"
      ],
      "metadata": {
        "id": "lhVt_kZSRHKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7: Transformer Model Building\n",
        "Defines a Transformer model architecture using Keras. It includes an embedding layer, a Transformer block with attention mechanisms, convolutional and pooling layers, and final dense layers.\n"
      ],
      "metadata": {
        "id": "6LaZyo_JVHTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Model Building\n",
        "embedding_dim = 32  # We can Adjust the embedding dimension based on our preference\n",
        "num_heads = 2\n",
        "ff_dim = 32\n",
        "\n",
        "inputs = Input(shape=(100,))\n",
        "embedding_layer = Embedding(input_dim=10000, output_dim=embedding_dim)(inputs)\n",
        "transformer_block = embedding_layer\n",
        "\n",
        "for _ in range(num_heads):\n",
        "    attn_output = Attention(use_scale=True)([transformer_block, transformer_block])\n",
        "    transformer_block = LayerNormalization(epsilon=1e-6)(transformer_block + attn_output)\n",
        "    transformer_block = Dropout(0.1)(transformer_block)\n",
        "\n",
        "transformer_block = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(transformer_block)\n",
        "transformer_block = tf.keras.layers.GlobalAveragePooling1D()(transformer_block)\n",
        "transformer_block = Dropout(0.1)(transformer_block)\n",
        "transformer_block = Dense(20, activation='relu')(transformer_block)\n",
        "output_layer = Dense(1, activation='sigmoid')(transformer_block)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=output_layer)"
      ],
      "metadata": {
        "id": "FTtkGY4vRJeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Model Training\n",
        "Compiles and trains the Transformer model on the training data for 5 epochs.\n"
      ],
      "metadata": {
        "id": "9L3Bc1XlVODK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_padded, y_train_encoded, epochs=5, validation_data=(X_test_padded, y_test_encoded))"
      ],
      "metadata": {
        "id": "r8CxdZamRMFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 9: Save the Model\n",
        "Saves the trained model in the HDF5 format for future use (while developing an app and its deployment)\n",
        "\n"
      ],
      "metadata": {
        "id": "H4ZwEP00VUoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/transformer_sentiment_model.h5')"
      ],
      "metadata": {
        "id": "I_eIXuGORNZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Evaluate the Model\n",
        "Evaluates the model on the test set, computes accuracy, and generates a classification report.\n"
      ],
      "metadata": {
        "id": "e0aa5l6mViAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_prob = model.predict(X_test_padded)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Converting back to original labels\n",
        "y_pred_original = label_encoder.inverse_transform(y_pred.flatten())\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test_encoded, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_encoded, y_pred))"
      ],
      "metadata": {
        "id": "V3oxW0a2ROlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: User Input Prediction\n",
        "Defines a function for predicting sentiment based on user input and prompts the user to input a review for sentiment prediction.\n"
      ],
      "metadata": {
        "id": "bA8aqwlIVu1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Input Prediction\n",
        "def predict_sentiment_transformer(review):\n",
        "    cleaned_review = re.sub(r'[^a-zA-Z\\s]', '', review.lower())\n",
        "    cleaned_review = ' '.join(word for word in cleaned_review.split() if word not in stop_words)\n",
        "    sequence = tokenizer.texts_to_sequences([cleaned_review])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=100, truncating='post', padding='post')\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    if 0.4 <= prediction <= 0.55:\n",
        "        return \"Neutral\"\n",
        "    elif prediction > 0.55:\n",
        "        return \"Positive\"\n",
        "    else:\n",
        "        return \"Negative\"\n",
        "\n",
        "# Enter your review to get the prediction of the sentiment\n",
        "user_input = input(\"Enter your Review here: \")\n",
        "prediction = predict_sentiment_transformer(user_input)\n",
        "print(f\"Predicted Sentiment: {prediction}\")"
      ],
      "metadata": {
        "id": "kJP4ihNSRSkx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}